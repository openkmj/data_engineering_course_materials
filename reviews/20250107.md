## 스크럼

- 생각해볼것
  - 내 코드는 pandas dataframe의 장점을 활용하고 있는가
  - 외부의 변화(웹페이지 변경 등)에 대한 대비
- M3 추가 구현
  - sqlite

## 오늘 한 일

- 생각해보기
  - 내 코드는 pandas dataframe의 장점을 활용하고 있는가
    - list를 만들고 dataframe을 만들기 vs 매 iteration마다 dataframe에 concat하기
      - 매번 dataframe concat하면 성능이 안 좋다.
      - 그런데 list에 넣기 어려울 정도로 데이터 양이 많다면?
        1. chunk 단위로 list를 나누고 dataframe을 만들어서 concat
        2. file read 할 때 generator를 사용
    - 가능한 모든 transform은 dataframe 내에서 처리하자.
  - 외부의 변화(웹페이지 변경 등)에 대한 대비
    - 데이터 소스가 다양할 수 있다! (웹, 파일, db, ...)
    - 데이터 저장 방법도 다양할 수 있다! (json, csv, db, ...)
    - interface와 implementation은 분리하자.
      - 의도: 모든 importer, exporter는 동일한 interface를 갖는다.
        - > 유지보수 용이.
      - importer: 원본 데이터를 가져와서 dataframe으로 반환
      - exporter: dataframe을 대상 저장소에 저장
      - importer, exporter interface를 만들고 이를 상속받아 자식클래스에서 로직 구현
- 코드 가독성 개선
  - Logger, Importer, Exporter 파일 분리
- 성능 테스트
  - 데이터 양을 늘려서 테스트 해보자
  - 더미 csv 파일로 ETL 각 단계별 시간 측정
    - 10M row(260MB), 100M row(2.6GB)
  - 인사이트
    ### Transform
    ```python
    # 10M: 4.79s
    df["GDP"] = df["GDP"].apply(lambda x: x.replace(",", "")) # 1.08s
    df["GDP"] = df["GDP"].apply(lambda x: round(float(x) / 1000, 2)) # 3.71s
    ```
    ```python
    # 10M: 4.24s
    df["GDP"] = df["GDP"].apply(lambda x: round(float(x.replace(",", "")) / 1000, 2))
    ```
    두 코드가 같은 결과를 내지만 유의미한 성능 상 차이가 있었다. 11%면 유의미한 차이.
    why? pandas는 spark처럼 optimizer가 있는게 아님.


    다른 메서드 사용해보기
    ```python
    # 10M: 4.06s
    df["GDP"] = (
        pd.to_numeric(df["GDP"].str.replace(",", ""), errors="coerce")
        .div(1000)
        .round(2)
    )
    ```
    ```python
    # 10M: 3.23
    df["GDP"] = (df["GDP"].replace(",", "", regex=True).astype(float) / 1000).round(2)
    ```
    ```python
    # 10M: 1.66s
    # 100M: 20.9s
    df["GDP"] = (df["GDP"].str.replace(",", "").astype(float) / 1000).round(2)
    ```
    같은 결과를 내는 코드라도 성능이 다르다.

    ### Extract
    ```python
    # 10M: 4.51s
    # 100M: 48.77s
    df = pd.read_csv("large_data.csv")
    ```
    파일이 커지면 메모리가 모자라서 한번에 읽을 수 없을 것 같다.
    chunk 단위로 나눠서 읽어보자. 이때 성능이 어떻게 변할까?
    ```python
    chunks = pd.read_csv(
        "large_data.csv",
        dtype=schema,
        header=None,
        names=schema.keys(),
        chunksize=CHUNKSIZE,
    )
    df = pd.concat(chunks)
    ```
    Chunk size에 따른 성능 차이
    |               | chunksize 10K | chunksize 100K | chunksize 1M | none   |
    | ------------- | ------------- | -------------- | ------------ | ------ |
    | datasize 10M  | 4.82s         | 3.92s          | 4.3s         | 4.51s  |
    | datasize 100M | 46.85s        | 40.25s         | 44.38s       | 48.77s |

    100M(2.6GB) 데이터로는 메모리 부족 문제를 볼 순 없었다.

    예상:
    - chunk가 여러개로 나눠질수록(chunksize가 작아질수록) 속도가 줄어들 것이다.
      - i/o 오버헤드, dataframe을 새로 만드는 오버헤드, concat하는 오버헤드
  
    결과:
    - 데이터 크기와 상관없이 chunksize 100K(26MB)가 가장 빨랐다.
      - 왜 파일을 한번에 읽는 것보다 100K 단위로 읽는 것이 더 빠를까?
      - why...? 전혀 모르겠다.
        - 블록사이즈?
      - 메모리 할당 문제?
        - pandas 내부적으로 사용하는 read buffer가 있는데 그 크기를 chunksize를 바탕으로 설정한다.
        - buffer가 읽을 데이터보다 작으면 realloc한다.
        - 너무 크면 realloc이 자주 발생해서 메모리 복사 오버헤드가 발생할 수 있다..?
      - 정확한 원인 파악을 위해선 속도와 함께 메모리 사용량이나 disk io도 봐야할 것 같다.
  
  

## KPT

### Keep

- 유지보수를 위해 코드를 개선할 때 명확한 의도를 먼저 정의했던 것이 좋았다.
  - 코드 리팩토링 할 때는 먼저 의도를 정확히 하자.
- 성능 개선을 위해 여러가지 실험을 해본것이 좋았다.
- 실험 결과를 GPT를 통해 분석해보는것이 좋았다.
  - 왜 이런 결과가 나왔는지 이해하는 데 도움이 됐다.
- 팀과 코드를 공유하면서 피드백을 받는것이 좋았다.
  - 다른 사람의 코드를 보며 같은 결과를 내는 다른 코드를 비교해볼 수 있었다.

### Problem

- 에러 핸들링, 가시성에 대한 고민이 부족했다.
- 속도만 측정해서 그런지 원인 파악이 어려웠다.

### Try

- 시간이 오래걸리더라도 더 큰 데이터로 테스트해볼 수 있으면 좋겠다.
- 속도말고 다른 지표도 측정해보자.
- 병렬처리를 해보자.