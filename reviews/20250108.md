## 스크럼

- 생각해볼것
  - ETL 각 단계의 정의
  - ETL 각각의 과정을 별도로 프로세스로 처리한다면?
  - 병렬/분산 처리를 한다면 어떻게 바꿔야할까?
  - 에러를 어떻게 대응할지?
- M3 최적화
  - 병렬처리 구현

## 오늘 한 일

- 생각해보기
  - ETL 각 단계의 정의
    - E: 여러 소스로부터 데이터를 읽어서 structured data로 변환하고 workspace로 가져오기
      - 웹에서 크롤링하고 region을 넣는 과정은 별도의 ETL 과정으로 나눌 수도 있음.
    - T: GDP를 변환하고 내림차순으로 정렬
    - L: sqlite로 .db에 저장하기
    - 팀의 결론: 정의하기 나름. 팀원들이 모두 같은 기준을 가지고 있으면 된다.
  - ETL 각각의 과정을 별도로 프로세스로 처리한다면?
    - 프로세스간의 데이터 교환을 어떻게 할지?
    - 어차피 에러 대응을 위해서 중간 데이터를 저장해야한다.
  - 병렬/분산 처리를 한다면 어떻게 바꿔야할까?
    - 요구사항이 region별 groupby 연산이니 데이터가 저장될 때 region별로 파티셔닝이 되면 좋겠다.
    - region별로 데이터 분포가 다르다면 어떻게 효율적으로 작업을 처리할지..?
  - 에러를 어떻게 대응할지?
    - 어느 지점까지 완료됐는지를 기록하고
    - 중간부터 다시 시작할 수 있어야 한다.
- 병렬 처리 구현
  - 비즈니스 요구사항: 쿼리를 빠르게!
    - GDP 칼럼을 기준으로 필터링과 정렬이 자주 발생한다.
    - groupby region이 자주 발생한다.
    -> region을 기준으로 파티셔닝하고 GDP 기준으로 정렬하여 저장.
  - 구현 요구사항
    - ETL 각 단계를 독립적으로 구현
      - 로깅을 통해 실패한 지점을 찾을 수 있어야 한다.
      - 매 단계별로 중간 데이터를 저장한다.
    - 각 단계는 멀티프로세스로 구현한다.
    - 분산환경을 가정해서 프로세스간 데이터 교환은 파일로 하자.

    1. (Extract) 하나의 큰 파일을 여러 개의 작은 파일로 분할: data.csv -> data_0.csv, data_1.csv, ...
    2. (Transform - preprocess) 각 작은 파일을 읽고 string to float 변환: data_0.csv -> data_0_preprocessed.csv, data_1_preprocessed.csv, ...
    3. (Transform - map) 데이터를 region별로 분할: data_0_preprocessed.csv -> data_0_asia.csv, data_0_europe.csv, ...
        - why mapreduce? data_asia.csv로 바로 append하면 안될까?
          - 프로세스/머신 수가 많아진다면 효율이 확 떨어짐!
    4. (Transform - reduce) region별로 합치기: data_0_asia.csv, data_1_asia.csv, ... -> data_asia.csv
    5. (Transform - sort) GDP 기준으로 정렬: data_asia.csv -> data_asia_sorted.csv
    6. (Load) sqlite에 region별로 따로 저장: data_asia_sorted.csv -> data_asia_sorted.db
    7. (Query) 리전 별로 각각 다른 디비에 쿼리

- 고민
  - Chunk 크기를 어떻게 정하는 것이 좋은가.
  - CSV 파일 읽을 때 병렬 처리 문제
    - 매번 파일을 처음부터 읽어야해서 Disk I/O가 너무 크다.
      - why? csv는 각 row의 크기가 가변적이라 특정 row로 바로 접근할 수 없고 처음부터 읽어야한다.
        - Chunk 개수가 많아질수록 disk read 부하가 증가.
    - 애초에 특정 row로 바로 접근할 수 있는 데이터 포맷으로 저장하면 좋을 것 같다. -> parquet? DB?
      - 하나의 큰 파일보단 여러 개의 작은 파일로 나눠서 저장하는 것이 더 효율적이다.
    - 결론:
      - csv는 한 프로세스에서만 read하는게 효율적일 것 같다.
      - 큰 파일은 csv로 저장하지 말자.
  - region별로 데이터를 나눈 것이 과연 효율적인가?
    - 쿼리 시에 각 region별로 따로 요청하고 합치는 과정이 필요하다.
      - 지금 고려한 요구사항에서는 괜찮다.
      - region 상관없이 전체 데이터를 쿼리하는 경우가 많다면 효율적이지 않을 것 같다.
        - ex) GDP 상위 100개 국가 등등
    - 요구사항에 맞게 적절한 데이터 저장 구조를 선택해야한다.


## KPT

### Keep

- 새로운 라이브러리(multiprocessing)를 사용해본 것이 좋았다.

### Problem

- multiprocessing을 정확히 이해하고 사용하진 못했다.
- 여러가지 변수가 추가되면서 병렬처리가 얼마나 효율적인지 파악하기 어려웠다.
  - 중간 데이터 저장하기.
  - mapreduce 단계 추가.

### Try

- python multithreading과 multiprocessing 비교해보기
