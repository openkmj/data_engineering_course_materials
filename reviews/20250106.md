## 스크럼

- M3 단계별로 진행하고 중간중간 공유.

## 오늘 한 일

- M3 코드 구현
  - IMF GDP 데이터 ETL 프로세스 구현
- M3 팀활동
  - wikipeida 페이지가 아닌, IMF 홈페이지에서 직접 데이터를 가져오는 방법은 없을까요?
    - API 요청을 통해 가져올 수 있었다.
  - 만약 데이터가 갱신되면 과거의 데이터는 어떻게 되어야 할까요? 과거의 데이터를 조회하는 게 필요하다면 ETL 프로세스를 어떻게 변경해야 할까요?
    - 날짜 별로 다른 파일에 저장하자.
    - 날짜에 해당하는 칼럼을 추가하고 같은 테이블에 계속 쌓자.
  - Raw 데이터의 양이 압도적으로 많거나 Transform 과정이 오래 걸린다면?
    - 내 생각
      - 어떤 문제가 있을까?
        - (성능) 오래 걸린다. 느리다. 
          - 파티셔닝, 병렬처리
          - 데이터를 쪼개서 html 파싱, transform 과정을 병렬로 처리할 수 있다.
            - 단일 html 파일이 무지하게 크다면?
              - Raw 데이터를 일단 먼저 저장하고 디스크에서 가져다쓰자.
          - json 파일로 데이터 저장 시 파티션 별로 따로 저장한다.
            - 하나의 파일을 여러 워커가 본다면 성능 상의 문제가 생길 수 있다.
            - > 특정 파티션에 문제가 있을 때 해당 파티션만 다시 실행하면 된다.
            - data use case에서 region으로 groupby를 하는 경우가 많으니 파티션은 region 별로 나누면 좋겠다!
          - API로 요청하는 경우
            - Extract 과정(API 요청)을 병렬로 처리하는 게 좋다.
              - API 요청 수가 매우 많아지면 결국 병목이 생길지도.
              - 적당한 batch size를 조절하는 것이 필요하겠다.
        - (에러 처리) 한번 실행할 때 오래 걸리니 실패 시 손해가 더 커진다.
          - 실패했을 때 처음부터 다시 실행하지 말고, 실패한 지점부터 실행하면 좋겠다.
            - 단계별로 intermediate 데이터를 저장하기.
          - 문제가 생겼음을 즉시 알 수 있어야한다.
        - 언제 끝날 지 모른다.
            - 지금 얼마나 진행됐는지도 볼 수 있으면 좋겠다.
      - 데이터 양이 많은 경우, transform이 오래 걸리는 경우. 두가지가 어떻게 다른가?
        - 결국 같은 문제상황 아닐까?
        - 해결방법이 같은 것 같다.. 잘 모르겠다.
    - 팀의 생각
      - Extract 과정을 병렬로 처리하기 위해 파일을 저장할때도 파티셔닝해서 저장하자.
      - Transform이 오래 걸리는 경우에는 Transform 과정을 여러개의 Transform 과정으로 쪼개서 관리하면 좋다.
        - 하나의 큰 ETL을 작은 여러개의 ETL로 처리
          - > 에러 처리와 복구 용이성.
          - 그렇다면 무엇을 기준으로 Transform 작업을 나눠야할까..?
            - 생각해보기..
  
## KPT

### Keep

- 팀과 코드리뷰를 진행하는 것이 좋았다.
  - 더 깔끔한 코드와 패턴을 공유할 수 있었다.
- 생각해볼 거리들을 팀원들과 계속 얘기하는 것이 좋았다.

### Problem

- 없음.

### Try

- 아주 큰 데이터를 가정해서 더 고민해보기.
  - 파이썬 멀티 쓰레드 사용해보기.
- 에러 상황을 가정하고 복구하는 방법을 고민해보기.
